\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{csquotes}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\pprime}{\prime \prime}
\newcommand{\BigO}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\Id}{\mathbb{1}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}

\begin{document}

\title{CS6210: Homework 3}
\author{Christopher Mertin}
\date{October 4, 2016}
\maketitle

\begin{enumerate}
%%%%%% Problem 1 %%%%%%
\item The {\em condition number} of an eigenvalue $\lambda$ of a given matrix $A$ is defined as

\[
s(\lambda) = \frac{1}{\mathbf{x}^{T}\mathbf{w}}
\]

where $\mathbf{x}$ is a (right) eigenvector of the matrix, satisfying $A\mathbf{x} = \lambda \mathbf{x}$,
and $\mathbf{w}$ is a left eigenvector, satisfying $\mathbf{w}^{T}A = \lambda \mathbf{w}^{T}$.
Both $\mathbf{x}$ and $\mathbf{w}$ are assumed to have a unit $\ell_{2}$-norm. Loosly speaking,
the condition number determines the difficulty of computing the eigenvalue in question accurately;
the smaller $s(\lambda)$ is, the more numerically stable the computation is expected to be.

Determine the condition number of the eigenvalue 4 for the two matrices discussed in Example 4.7.
Explain the meaning of your results and how they are related to the observations made in the example.

{\bf Solution:}

\begin{align*}
\intertext{In Example 4.7, the given matrix and its eigenvalue 4 is}
A &= \begin{pmatrix}
    4 & 1\\
    0 & 4
\end{pmatrix};\quad\quad \mathbf{x} = \begin{pmatrix}1\\ 0\end{pmatrix};\quad\quad \mathbf{w}=\begin{pmatrix}0\\ 1\end{pmatrix}
\end{align*}

\begin{align*}
\intertext{In calculating the condition number, we get}
S(\lambda) &= \frac{1}{\mathbf{x}^{T}\mathbf{w}}\\
           &= \frac{1}{0} = \infty
\end{align*}

This means that the matrix is {\em ill-conditioned} and is the reason for brining out the
large discrepancy in eigenvalues with such a small perturbation.

%%%%%% Problem 2 %%%%%%
\item The {\em Gauss-Jordan method} used to solve the prototype linear system can be described
as follows. Augment $A$ by the right-hand-side vector $\mathbf{b}$ and proceed as in Gaussian Elimination,
except use the pivot element $a_{k,k}^{(k-1)}$ to eliminate not only $a_{i,k}^{(k-1)}$ for $i = \{ k+1, \ldots, n\}$
but also the elements $a_{i,k}^{(k-1)}$ for $i = \{1, \ldots, k-1\}$, {\em i.e.}, all elements in the $k^{th}$ column
other than the pivot. Upon reducing $(A|\mathbf{b})$ into

\[
\left[
\begin{array}{cccc|c}
  a_{1,1}^{(n-1)} & 0 & \cdots & 0 & b_{1}^{(n-1)}\\
  0 & a_{2,2}^{(n-1)} & \ddots & \vdots & b_{2}^{(n-1)}\\
  \vdots & \ddots & \ddots & 0 & \vdots\\
  0 & \cdots & 0 & a_{n,n}^{(n-1)} & b_{n}^{(n-1)}
\end{array}\right]
\]

the solution is obtained by setting

\[
x_{k} = \frac{b_{k}^{(n-1)}}{a_{k,k}^{(n-1)}},\quad k = \{ 1, \ldots, n\}
\]

This procedure circumvents the backward substitution part necessary for the Gaussian Elimination algorithm.

\begin{enumerate}
\item Write a pseudocode for this Gauss-Jordan procedure using, {\em e.g.}, the same format
as for the one appearing in Section 5.2 for Gaussian Elimination. You may assume that no
pivoting ({\em i.e.}, no row interchanging) is required.

{\bf Solution:}

\begin{algorithm}[H]
\caption{Gauss-Jordan Elimination}
\begin{algorithmic}[1]
\REQUIRE {$A\in \mathbb{R}^{n\times m}$}
\ENSURE $\vec{x}\in \mathbb{R}^{n\times 1}$
\FOR{$i = 1,\ldots ,n$}
    \STATE Let $p$ be the smallest integer with $i \leq p \leq n$ and $a_{p,i}\neq 0$
    \IF{no integer $p$ can be found}
          \STATE{\textbf{Return} `No unique solution exists'}
    \ENDIF
    \IF{$p\neq i$}
        \STATE $E_{p} \leftrightarrow E_{i}$
    \ENDIF
    \FOR{$j = 1,\ldots , i-1, i+1, \ldots,  m$}
      \STATE $m_{j,i} = \frac{a_{j,i}}{a_{i,i}}$
      \STATE $(E_{j} - m_{j,i}\cdot E_{i}) \rightarrow E_{j}$
    \ENDFOR
\ENDFOR
\COMMENT{Elimination Process}
\FOR{$i = 1,\ldots,n$}
    \STATE $x_{i} = \frac{a_{i,n+1}}{a_{i,i}}$
\ENDFOR
\STATE {\textbf{Return} $\vec{x}$}
\end{algorithmic}
\end{algorithm}

\item Show that the Gauss-Jordan method requires $n^{3} + \BigO{n^{2}}$ floating point operations
for one right-hand-side vector $\mathbf{b}$ -- roughly 50\% more than what's needed for Gaussian Elimination

{\bf Solution:}

This can be easily proven (assuming the given matrix is $n\times n$). Line 1
and 9 of the code produce $\BigO{n^{2}}$ divisions for line 10. However, Line 11
has to sub in the elements of each row ($\BigO{n}$), which would make that term
be $\BigO{n^{3}}$ since it has the two for loops, plus iterating over each element
in the columns to change their values.


\end{enumerate}

%%%%%% Problem 3 %%%%%%
\item Let $A$ and $T$ be two nonsingular $n \times n$ real matrices. Furthermore, suppose we are given
two matrices $L$ and $U$ such that $L$ is the unit lower triangular, $U$ is the upper triangular, and

\[
TA = LU
\]

Write an algorithm that will solve the problem

\[
A\mathbf{x} = \mathbf{b}
\]

for any given vector $\mathbf{b}$ in $\BigO{n^{2}}$ complexity. First, explain briefly yet clearly
why your algorithm requires only $\BigO{n^{2}}$ flops (you may assume without proof that solving an upper triangular
or a lower triangular system requires only $\BigO{n^{2}}$ flops). Then, specify your alogirhtm in detail
(including the details for lower and upper triangular systems) using pseudocode or a {\sc Matlab} script.

{\bf Solution:}

By multiplying $T$ from the left to the equation $A\mathbf{x} = \mathbf{b}$ gives us
$TA\mathbf{x} = T\mathbf{b} = \mathbf{c}$ and $LU\mathbf{x} = \mathbf{c}$. This can be
solved with normal LU-Decomposition, by first doing a forward and then a backward substitution.
Multiplying $T$ to vector $b$ needs $\BigO{n^{2}}$ operations, and since solving the
upper triangular or lower triangular system requires only $\BigO{n^{2}}$ flops, solving this
equaiton needs $\BigO{n^{2}}$ flops overall.


\begin{algorithm}[H]
\caption{LU Decomposition}
\begin{algorithmic}[1]
\STATE $\mathbf{c} = T\mathbf{b}$
\COMMENT{Multiply $T$ from the left on $A\mathbf{x} = \mathbf{b}$, with RHS as $\mathbf{c}$}
\STATE $\mathbf{y} = \mathbf{c}$
\FOR{$i = 2,\ldots,n$}
    \FOR{$j = 1,\ldots,i-1$}
        \STATE{$y_{i} = c_{i} - L_{i,j} y_{j}$}
    \ENDFOR
\ENDFOR
\COMMENT{Solve $L\mathbf{y} = \mathbf{b}$ via forward substitution}
\STATE $\mathbf{x} = \mathbf{y}$
\STATE $x_{n} = \frac{y_{n}}{U_{n,n}}$
\FOR{$i = n-1,\ldots,1$}
    \FOR{$j = i + 1,\ldots,n$}
      \STATE $x_{i} = \frac{y_{i} - U_{i,j}}{U_{i,i}}$
    \ENDFOR
\ENDFOR
\COMMENT{Solve $U\mathbf{x} = \mathbf{y}$ by backward substitution}
\end{algorithmic}
\end{algorithm}

%%%%%% Problem 4 %%%%%%
\item The classical way to invert a matrix $A$ in a basic linear algebra course augments $A$ by the $n\times n$
identity matrix $\Id$ and applies the Gauss-Jordan algorithm of Exercise 2 to this augmented matrix
(including the solution part, {\em i.e.}, the division by the pivots $a_{k,k}^{(n-1)}$). Then $A^{-1}$
shows up where $\Id$ initially was.

How many floating point operations are required for this method? Compare this to the operation count of
$\frac{8}{3}n^{3} + \BigO{n^{2}}$ required for the same task using LU-decomposition (see Example 5.5).

{\bf Solution:}

To add (agument) $\Id$ to $A$ in order to find the inverse, this takes $\BigO{n}$ operations.
Following this, we can convert $A$ into $\Id$ by utilizing row operations, where the {\em former}
matrix $\Id$ on the right becomes $A^{-1}$.

We can count up the operations in Algorithm~\ref{alg:inv}, to get the total number of operations,
which results in:

\[
\sum_{k=1}^{n} \left(2n + 2(2n-1)^{2} + 4n\right) + \BigO{n^{2}} = 8n^{3} + \BigO{n^{2}}
\]


\begin{algorithm}[H]
\caption{Matrix Inverse}
\begin{algorithmic}[1]
\FOR{$i = 1,\ldots,n$}
    \FOR{$j = 1,\ldots,2n$}
      \STATE $l_{j,i} = \frac{a_{j,i}}{a_{i,i}}$
      \FOR{$k = 1,\ldots,2n$}
          \IF{$i \neq j$}
              \STATE $a_{j,k} = a_{j,k} - l_{j,i}a_{i,k}$
          \ENDIF
      \ENDFOR
      \STATE $b_{j} = b_{j} - l_{j,i}b_{i}$
    \ENDFOR
\ENDFOR
\FOR{$i = 1,\ldots,n$}
  \STATE{$b = a_{i,i}$}
  \FOR{$j = 1,\ldots,2n$}
    \STATE{$a_{i,j} = \frac{a_{i,j}}{b}$}
  \ENDFOR
\ENDFOR
\COMMENT{Converting the left matrix to $\Id$}
\end{algorithmic}
\label{alg:inv}
\end{algorithm}

%%%%%% Problem 5 %%%%%%
\item The Cholesky algorithm given on page 116 has all those wretched loops as in the Gaussin Elimination
algorithm in its simplest form. In view of Section 5.4 and the program {\tt ainvb} we should be able to
achieve also the Cholesky decomposition effect more efficiently.

Write a code implementing the Cholesky decomposition with only one loop (on $k$), utilizing outer products.

{\bf Solution:}

\begin{algorithm}[H]
\caption{Cholesky Factorization (Outer Products)}
\begin{algorithmic}[1]
\FOR{$i = 1,\ldots,n$}
    \STATE $A_{i,i} = \sqrt{A_{i,i}}$
    \FOR{$j = i+1,\ldots, n$}
      \STATE $A_{j,i} = \frac{A_{j,i}}{A_{i,i}}$
    \ENDFOR
    \FOR{$j = i+1,\ldots,n$}
      \FOR{$k = j,\ldots,n$}
        \STATE{$A_{k,j} = A_{k,j} - A_{k,i}A_{j,i}$}
      \ENDFOR
    \ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg:chol}
\end{algorithm}

%%%%%% Problem 6 %%%%%%
\item Consider the LU decomposition of an upper Hessenberg (no, it's not a place in Germany) matrix,
defined on the facing page, assuming that no pivoting is needed: $A = LU$.

\begin{enumerate}
\item Provide an efficient algorithm for this LU decomposition (do not worry about questions of memory access and vectorization).

{\bf Solution:}

\begin{algorithm}[H]
\caption{LU Decomposition of Upper Hessenberg}
\begin{algorithmic}[1]
\FOR{$i = 1,\ldots,n-1$}
\STATE $j = i+1$
\COMMENT{Since $p = 2$, the second loop on the iterator $j$ only has a single value}
\STATE $l_{j,i} = \frac{a_{j,i}}{a_{i,i}}$
\FOR{$k = i+1,\ldots,i+n-1$}
    \STATE{$a_{j,k} = a_{j,k} - l_{j,i}a_{i,k}$}
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{alg:lu_hess}
\end{algorithm}

\item What is the sparsity structure of the resulting matrix $L$ ({\em i.e.}, where are its non-zeros)?

{\bf Solution:}

In general, $L$ is a lower-triangular matrix and has a lower-bandwidth of $p = 2$ and
an upper-bandwidth of $q = n$. Therefore, we can use the ``LU Decomposition for Banded Matrices''
on page 120 in the book, which is stated as Algorithm~\ref{alg:lu_hess} above.

\item How many operations (to a leading order) does it take to solve a linear system $A\mathbf{x} = \mathbf{b}$,
where $A$ is upper Hessenberg?

{\bf Solution:}

Line 5 in Algorithm~\ref{alg:lu_hess} has $(n-2)$ multiplications and subtractions, resulting
in a total of $2(n-2)$. Therefore, the total number of operations can be found as

\[
\sum_{k=1}^{n-1}2(n-2) = 2(n-1)(n-2) = \BigO{n^{2}}
\]

Where the total time for the decomposition is $\BigO{n^{2}}$, for the forward substitution is $\BigO{n^{2}}$,
and for the backward substitution is $\BigO{n^{2}}$. Thus, the resulting overall number of
operations is $\BigO{n^{2}}$.

\item Suppose now that partial pivoting is applied. What are the sparsity patterns of the factors of $A$?

{\bf Solution:}

Page 121 of the text states:

\begin{displayquote}
``If the usual row partial pivoting is applied, then the band structure may be altered
due to row interchanges. Using such pivoting, the upper-bandwidth $q$ of $U$ may increase
to $q + p - 1$.''
\end{displayquote}

In any Hessenberg Matrix, since $p = 2$ and $q = n$, the upper-bandwith will stay the
same and partial pivoting does not have any significant effect on the sparsity of it.

\end{enumerate}

\item For the arrow matrices of Example 5.15, determine the overall storage and flop count requirements
for solving the systems with $A$ and with $B$ in the general $n \times n$ case.

{\bf Solution:}

Converting matrix $A$ to $B$ can be done with an implementation of a row and column swap,
requiring us to find the flop count for $B$. $B$ can be converted into an upper/lower-triangular
matrix in $\BigO{n^{2}}$ flops, with $(n-1)$ elementary operations. From here, we can solve the
linear system with backwards/forwards substitution, which requires $\BigO{n^{2}}$ flops. Therefore,
the total number of operations is $\BigO{n^{2}}$.

The storage requirements can be determined by only storing the non-zeros of the matrix.
In the case of an {\em arrow matrix} from Example 5.15, the {\em max} is $\BigO{n}$
as it would be $\BigO{n}$ along the diagonal, and $\BigO{2n}$ on the ``arrow head''
part of the matrix, resulting in an overall storage of $\BigO{n}$.

\end{enumerate}

%\begin{proof}
%Blah, blah, blah.  Here is an example of the \texttt{align} environment:
%Note 1: The * tells LaTeX not to number the lines.  If you remove the *, be sure to remove it below, too.
%Note 2: Inside the align environment, you do not want to use $-signs.  The reason for this is that this is already a math environment. This is why we have to include \text{} around any text inside the align environment.
%\begin{align*}
%\sum_{i=1}^{k+1}i & = \left(\sum_{i=1}^{k}i\right) +(k+1)\\
%& = \frac{k(k+1)}{2}+k+1 & (\text{by inductive hypothesis})\\
%& = \frac{k(k+1)+2(k+1)}{2}\\
%& = \frac{(k+1)(k+2)}{2}\\
%& = \frac{(k+1)((k+1)+1)}{2}.
%\end{align*}
%\end{proof}

\end{document}
