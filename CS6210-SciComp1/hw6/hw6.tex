\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{csquotes}
\usepackage{url}
\usepackage{enumerate}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\pprime}{\prime \prime}
\newcommand{\BigO}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\proj}[2][]{\textit{proj}_{\vect{#1}}\vect{#2}}
\newcommand{\vect}{\mathbf}
\newcommand{\Id}{\mathbb{1}}
\newcommand{\inv}[1]{ #1^{-1}}
\newcommand{\minn}{\text{min}}
\newcommand{\maxx}{\text{max}}
\renewcommand{\P}[1]{\left( #1 \right)}
\newcommand{\diag}[1]{\text{diag}\P{#1}}
\newcommand{\grad}{\nabla}
\newcommand{\laplacian}{\nabla^{2}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}

\begin{document}

\title{CS6210: Homework 6}
\author{Christopher Mertin}
\date{December 8, 2016}
\maketitle

\begin{enumerate}
%%%%%% Problem 1 %%%%%%
\item 

\begin{enumerate}
\item Using an orthogonal polynomial basis, find the best least squares polynomial approximations, $q_{2}(t)$ of degree at most 2 and $q_{3}(t)$ of degree at most 3, to $f(t) = e^{-3t}$ over the interval $[0,3]$.

[Hint: For a polynomial $p(x)$ of degree $n$ and a scalar $a > 0$ we have $\int e^{-ax}p(x)\text{d}x = -\frac{e^{-ax}}{a}\ \left( \sum_{j=0}^{n} \frac{p^{(j)}(x)}{a^{j}}\right)$, where $p^{(j)}(x)$ is the $j^{th}$ derivative of $p(x)$. Alternatively, just use numerical quadrature, e.g., the {\sc Matlab} function {\tt quad}.]

{\bf Solution:}

\item Plot the error functions $f(t) - q_{2}(t)$ and $f(t) - q_{3}(t)$ on the same graph on the interval $[0,3]$. Compare the errors of the two approximating polynomials. In the least squares sense, which polynomial provides the better approximation?

[Hint: In each case you may compute the {\em norm} of the error, $\left( \int_{a}^{b}(f(t) - q_{n}(t))^{2}\text{d}t \right)^{2}$, using the {\sc Matlab} function {\tt quad}]

{\bf Solution:}

\item Without any computation, prove that $q_{3}(t)$ generally provides a least squares fit, which is never worse than with $q_{2}(t)$. 

{\bf Solution:}
\end{enumerate}

\item Let $f(x)$ be a given function that can be evaluated at points $x_{0} \pm jh,\ j=\{0,1,2,\ldots\}$ for any fixed value of $h$, $0 < h \ll 1$.

\begin{enumerate}
\item Find a second order formula (i.e., trunctation error $\BigO{h^{2}}$) approximating the third derivative $f^{\prime \prime \prime}(x_{0})$. Give the formula, as well as an expression for the truncation error, i.e. not just its order

{\bf Solution:}

\item Use your formula to provide approximations to $f^{\prime \prime \prime}(0)$ for the function $f(x) = e^{x}$ employing values $h = \{ 10^{-1}, 10^{-2}, \ldots, 10^{-9}\}$, with the default ${\tt Matlab}$ arithmetic. Verify that for the larger values of $h$ your formula is indeed second order accurate. Which value of $h$ gives the closest approximation to $e^{0} = 1$?

{\bf Solution:}

\item For the formual that you derived in (a), how does the roundoff error behave as a function of $h$, as $h\rightarrow 0$.

{\bf Solution:}

\item How would you go about obtaining a forth order formula for $f^{\prime \prime \prime}(x_{0})$ in general? (You don't have to actually derive it: just describe in one or two sentences.) How many points would this formula require?

{\bf Solution:}
\end{enumerate}

\item Consider the derivation of an approximate formula for the second derivative $f^{\prime \prime}(x_{0})$ of a smooth function $f(x)$ using three points $x_{-1},\ x_{0}=x_{-1} + h_{0},\ \text{and}\ x_{1}=x_{0}+h_{1}$, where $h_{0} \neq h_{1}$. 

Consider the following two methods:

\begin{enumerate}[i.]
\item Define $g(x) = f^{\prime}(x)$ and seek a {\em staggered mesh}, centered approximiations as follows:

\begin{align*}
g_{1/2} &= \frac{f(x_{1}) - f(x_{0})}{h_{1}};\quad g_{-1/2} = \frac{f(x_{0}) - f(x_{-1})}{h_{0}}\\
f^{\prime \prime}(x_{0}) &\approx \frac{g_{1/2} - g_{-1/2}}{(h_{0} + h_{1})/2}
\end{align*}

The idea is that all of the differences are short(i.e., not long differences) and centered.

\item Using the second degree interpolating polynomial in Newton form, differentiated twice, define

\[
f^{\prime \prime}(x_{0}) \approx 2f[x_{-1}, x_{0}, x_{1}]
\]

\end{enumerate}

Here is where you come in:

\begin{enumerate}
\item Show that the above two methods are one in the same

{\bf Solution:}

\item Show that this method is only first order accurate in general

{\bf Solution:}

\item Run the two methods for the example depicted in Table 14.2 (but for the second derivative of $f(x) = e^{x}$). Report your findings

{\bf Solution:}

\end{enumerate}

\item Continuing with the notation of Exercise 12 (page 437), one could define 

\[
	g_{1/2} = \frac{f_{1} - f_{0}}{h}\quad \text{and}\quad g_{-1/2}=\frac{f_{0} - f_{-1}}{h}
\]

These approximate to second order the first derivative values $f^{\prime}(x_{0} + h/2)$ and $f^{\prime}(x_{0} - h/2)$, respectively. Then define

\[
	f_{pp_{0}} = \frac{g_{1/2} - g_{-1/2}}{h}
\]

All three derivative approximations here are centered (hence second order), and they are applied to first derivatives and hence have roundoff error increeasing proportionally to $h^{-1}$, not $h^{-2}$. Can we manage to (partially) cheat the hangman way?!

\begin{enumerate}
\item Show that in exact arithmetic $f_{pp_{0}}$ defined above and in Exercise 12 are one in the same

{\bf Solution:}

\item Implement this method and compare to the results of Exercise 12. Explain your observations

{\bf Solution:}
\end{enumerate}

\item Consider the numerical differentiation of the function $f(x) = c(x) e^{x/\pi}$ defined on $[0, \pi]$, where 

\[
	c(x) = j, \quad \frac{1}{4}(j-1)\pi \leq x < \frac{1}{4}j\pi, \quad j = \{ 1, 2, 3, 4\}
\]

\begin{enumerate}
\item Contemplating a difference approximation with step size $h = n/\pi$, explain why it is a very good idea to ensure that $n$ is an integer multiple of $4,\ n = 4l$. 

{\bf Solution:}

\item With $n = 4l$, show that the expression $h^{-1}c(t_{i})\left( e^{x_{i+1}/\pi} - e^{x_{i}/\pi}\right)$ provides a second order approximation (i.e., $\BigO{h^{2}}$ error) of $f^{\prime}(t_{i})$, where $t_{i} = x_{i} + h/2 = (i + 1/2)h, i = \{ 0, 1, \ldots, n-1\}$

{\bf Solution:}


\end{enumerate}

\item The basic trapezoidal rule for approximating $I_{f} = \int_{a}^{b}f(x)\text{d}x$ is based on linear interpolation of $f$ at $x_{0} = a$ and $x_{1} = b$. The Simpson rule is likewise based on quadratic polynomial interpolation. Consider now a cubic Hermite polynomial, interpolating both$f$ and its derivative $f^{\prime}$ at $a$ and $b$. The osculating interpolation formula gives

\[
	p_{3}(x) = f(a) + f^{\prime}(a)(x-a) + f[a,a,b,b](x-1)^{2}(x - b)
\]

and integrating this yields (after some algebra)

\[
	I_{f} \approx \int_{a}^{b} p_{3}(x)\text{d}x = \frac{b-a}{2}[f(a) + f(b)] + \frac{(b-a)^{2}}{12}[f^{\prime}(a) - f^{\prime}(b)]
\]

This formula is called the {\bf corrected trapezoidal rule}

\begin{enumerate}
\item Show that the error in the basic corrected trapezoidal rule can be estimated by

\[
	E(f) = \frac{f^{(4)}(\eta)}{720}(b-a)^{5}
\]

{\bf Solution:}

\item Use the basic corrected trapezoidal rule to evaluate approximations for $\int_{0}^{1}e^{x}\text{d}x$ and $\int_{0.9}^{1}e^{x}\text{d}x$. Compare errors to those of Example 15.2. What are your observations?

{\bf Solution:}

\end{enumerate}

\item 

\begin{enumerate}
\item Derive a formula for the {\em composite midpoint rule}. How many function evaluations are required?

{\bf Solution:}

\item Obtain an expression for the error in the composite midpoint rule. Conclude that this method is second order accurate

{\bf Solution:}
\end{enumerate}

\item Suppose that the interval of integration $[a, b]$ is divided into equal subintervals of length $h$ each such that $r = (b - a)/h$ is even. Denote by $R_{1}$ the result of applying the composite trapezoidal method with step size $2h$ and by $R_{2}$ the result of applying the same method with step size $h$. Show that one application of Richardson extrapolation, reading 

\[
S = \frac{4R_{2} - R_{1}}{3}
\]

yields the composite Simpson method

{\bf Solution:}

\item Using Romberg integration, compute $\pi$ to 8 digits (i.e. 3.xxxxxxxx) by obtaining approximations to the integral

\[
\pi = \int_{0}^{1}\left( \frac{4}{1 + x^{2}}\right)\text{d}x
\]

Describe your solution approach and provide the appropriate Romberg table.

Comapre the computational effort (function evaluations) of Romberg integration to that using the adaptive routine developed in Section 15.4 with {\tt tol}$= 10^{-7}$.

You may find for some of the rows of your Romberg table that only the first step of extrapolation improves the approximation. Explain this phenomenon.

[Hint: Reconsider the assumed form of the composite trapezoidal method's truncation error and the effects of extrapolation for this particular integration]

{\bf Solution:}

\end{enumerate}

%\begin{proof}
%Blah, blah, blah.  Here is an example of the \texttt{align} environment:
%Note 1: The * tells LaTeX not to number the lines.  If you remove the *, be sure to remove it below, too.
%Note 2: Inside the align environment, you do not want to use $-signs.  The reason for this is that this is already a math environment. This is why we have to include \text{} around any text inside the align environment.
%\begin{align*}
%\sum_{i=1}^{k+1}i & = \left(\sum_{i=1}^{k}i\right) +(k+1)\\
%& = \frac{k(k+1)}{2}+k+1 & (\text{by inductive hypothesis})\\
%& = \frac{k(k+1)+2(k+1)}{2}\\
%& = \frac{(k+1)(k+2)}{2}\\
%& = \frac{(k+1)((k+1)+1)}{2}.
%\end{align*}
%\end{proof}

\end{document}
