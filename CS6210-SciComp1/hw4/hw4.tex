\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{float}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{csquotes}
\usepackage{url}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left|\left| #1 \right|\right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\pprime}{\prime \prime}
\newcommand{\BigO}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\proj}[2][]{\textit{proj}_{\vect{#1}}\vect{#2}}
\newcommand{\vect}{\mathbf}
\newcommand{\Id}{\mathbb{1}}
\newcommand{\inv}[1]{ #1^{-1}}
\newcommand{\minn}{\text{min}}
\newcommand{\maxx}{\text{max}}
\renewcommand{\P}[1]{\left( #1 \right)}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\csch}{csch}

\begin{document}

\title{CS6210: Homework 1}
\author{Christopher Mertin}
\date{November 1, 2016}
\maketitle

\begin{enumerate}
%%%%%% Problem 1 %%%%%%
\item Often in practice, an approximation of the form

\[
u(t) = \gamma_{1}e^{\gamma_{2}t}
\]

is sought for a data fitting problem, where $\gamma_{1}$ and $\gamma_{2}$ are
constants. Assume given data $\left(t_{1}, z_{1}\right),\left(t_{2}, z_{2}\right),\ldots,\left(t_{m}, z_{m}\right)$
where $z_{i} > 0$, $i = \{ 1,2,\ldots,m\}$, and $m > 0$.

\begin{enumerate}
\item Explain in one brief sentence why the techniques introduced in the present
chapter cannot be directly applied to find this $u(t)$.

{\bf Solution:}

Because the equation is not linear in the coefficients $\gamma_{1}$ and $\gamma_{2}$.

\item Considering instead

\[
v(t) = \ln\left( u(t)\right) = \left(\ln\left( \gamma_{1}\right)\right) + \gamma_{2}t
\]

it makes sense to define $b_{i} = \ln\left( z_{i}\right)$, $i = \{ 1,2,\ldots,m\}$,
and then find coefficients $x_{1}$ and $x_{2}$ such that $v(t) = x_{1} + x_{2}t$
is the best least squares fit for the data

$\left(t_{1}, b_{1}\right),\left(t_{2}, b_{2}\right),\ldots,\left(t_{m}, b_{m}\right)$

Using this method, find $u(t)$ for the data

\begin{table}[H]
  \centering
\begin{tabular}{| c | c | c | c |}
  \hline
$i$ & 1 & 2 & 3\\
\hline
$t_{i}$ & $0.0$ & $1.0$ & $2.0$\\
\hline
$z_{i}$ & $e^{0.1}$ & $e^{0.9}$ & $e^{2}$\\
\hline
\end{tabular}
\end{table}

{\bf Solution:}

We can take the exponential of both sides, giving

\begin{align*}
u(t) &= e^{\gamma_{1} + t\gamma_{2}} = e^{\gamma_{1}}e^{\gamma_{2}}\\
\intertext{with}
A &= \begin{pmatrix}1 & 0\\1 & 1\\ 1 & 2 \end{pmatrix}\quad \mathbf{b} = \begin{pmatrix}1\\ 0.9\\ 2 \end{pmatrix}
\intertext{In finding $\mathbf{x}$ in $A\mathbf{x} = \mathbf{b}$, we get}
x &= \begin{pmatrix}0.05\\ 0.95\end{pmatrix}\\
u(t) &= e^{0.05}e^{0.95t} = a_{1}e^{ta_{2}}
\end{align*}

\end{enumerate}

%%%%%% Problem 2 %%%%%%
\item \
\begin{enumerate}
\item Why can't one directly extend the LU-decomposition to a long and skinny
matrix in order to solve least squares problems?

{\bf Solution:}

It only works with square matrices.

\item When writing $\mathbf{x} = A^{\dagger}\mathbf{b} = \cdots = R^{-1}Q^{T}\mathbf{b}$
we have somehow moved from the conditioning $\left[\kappa(A)\right]^{2}$ to the
conditioning $\kappa(A)$ through mathematical equalities. Where is the improvement
step hidden? Explain.

{\bf Solution:}

For normal equations we have

\begin{align*}
A^{T}A\vect{x} &= A^{T}\vect{b}\\
\kappa\P{A^{T}A} &= \kappa\P{R^{T}Q^{T}QR} = \kappa\P{R^{T}R}\\
                 &= \kappa^{2}(R) = \kappa\P{A^{2}}\\
\intertext{For QR Factorization and sinc $Q$ is orthogonal and can be omitted in $\kappa$}
\kappa\P{\inv{R}Q^{T}} &= \kappa\P{\inv{R}} = \kappa{R} = \kappa{A}
\intertext{Since}
\kappa{R} &= \norm{R} \norm{\inv{R}} = \norm{\inv{R}} \norm{R} = \kappa\P{\inv{R}}
\end{align*}


\end{enumerate}

%%%%%% Problem 3 %%%%%%
\item \
\begin{enumerate}
  \item Explain what may happen during the course of the Gram-Schmidt process if
  the matrix $A$ is rank deficient.

  {\bf Solution:}

  From page 158 of the text, it states that when $A$ is rank deficient, $r_{i,i}$
  will be zero for some $i$, and then in calculating $q_{i}$ will lead to a division
  by zero.

  \item Show that classical Gram-Schmidt and the modified version are mathematically
  equivalent.

  {\bf Solution:}

  Due to rounding errors, $q_{i}$'s obtained during classical Gram-Schmidt are
  often {\em not orthogonal}. Therefore, we can rewrite it as

  \begin{align*}
    q_{k} &= a_{k} - \proj[q_{1}]{a_{k}} - \proj[q_{2}]{a_{k}} - \cdots - \proj[q_{k-1}]{a_{k}}\\
    \intertext{With the modified version being written as}
    q_{k}^{(1)} &= a_{k} - \proj[q_{1}]{a_{k}}\\
                &\vdots\\
    q_{k}^{(k-2)} &= q_{k}^{(k-3)} - \proj[q_{k-2}]{\left(q_{k}^{(k-3)}\right)}\\
    q_{k}^{(k-1)} &= q_{k}^{(k-2)} - \proj[q_{k-1}]{\left(q_{k}^{(k-2)}\right)}\\
    \intertext{where, if we write $q_{k}^{(k-1)}$ recursively, we get back the classical version:}
    q_{k}^{(k-1)} &= q_{k}^{(k-2)} - \proj[q_{k-1}]q_{k}^{(k-2)}\\
                  &= \left(q_{k}^{(k-3)} - \proj[q_{k-2}]q_{k}^{(k-3)}\right) - \proj[q_{k-1}]{\left( q_{k}^{(k-3)} - \proj[q_{k-2}]{q_{k}^{(k-3)}} \right)}  \\
                  &= a_{k} - \proj[q_{1}]{a_{k}} - \proj[q_{2}]{a_{k}} - \cdots - \proj[q_{k-1}]{a_{k}} \qed
  \end{align*}

  \item Construct a $3\times 2$ example, using a decimal floating point system
  with a 2-digit fraction, in which modified Gram-Schmidt proves to be more
  numerically stable than the classical version.

  {\bf Solution:}

  A $3 \times 2$ matrix is not possible as both of these algorithms would produce
  the same result. This is due to the for loops, since both of these produce the same
  values for the first two columns in the matrix.
\end{enumerate}

%%%%%% Problem 4 %%%%%%
\item Let $\alpha$ be scalar, and consider the iterative scheme

\[
\mathbf{x}_{k+1} = \mathbf{x}_{k} + \alpha\left(\mathbf{b} - A\mathbf{x}_{k} \right)
\]

This is the gradient descent method with a fixed step size $\alpha$.

\begin{enumerate}
  \item If $A = M - N$ is the splitting associated with this method, state what $M$
  and the iteration matrix $T$ are.

  {\bf Solution:}

  According to page 182 of the text, we have that $\inv{M} = \alpha \Id$, so $M = \frac{1}{\alpha}\Id$,
  giving the iteration matrix $T$ as

  \begin{align*}
    T &= \Id - \inv{M}A\\
      &= \Id - \alpha A
  \end{align*}

  \item Suppose $A$ is symmetrix positive definite and its eigenvalues are
  $\lambda_{1} > \lambda_{2} > \cdots > \lambda_{n} > 0$.

  \begin{enumerate}
    \item Derive a condition on $\alpha$ that guarentees convergence of the
    scheme to the solution $\mathbf{x}$ for any initial guess.

    {\bf Solution:}

    \begin{align*}
      x_{k+1} &= x_{k}\left( \Id - \alpha A\right) + \alpha \vect{b}\\
      \intertext{If $x_{k}$ is an eigenvector of $A$, then}
      x_{k+1} &= \alpha \lambda_{i} x_{k} + \alpha \vect{b} = (1 - \alpha \lambda_{i})x_{k} + \alpha \vect{b}\\
      e_{k+1} &= e_{k}(1 - \alpha \lambda_{i})
    \end{align*}

    For the series to be convergent, we need

    \begin{align*}
      \frac{e_{k+1}}{e_{k}} &< 1\\
      \abs{1 - \alpha \lambda_{i}} &< 1
    \end{align*}

    We have two inequalities: $1 - \alpha \lambda_{i} < 1$ and $1 - \alpha \lambda_{i} > -1$, giving the following

    \begin{align*}
      1 - \alpha \lambda_{i} &< 1\\
      \Rightarrow \alpha &> 0\\
      \alpha &< \frac{2}{\lambda_{i}}
    \end{align*}

    If we put the biggest eigenvalue, $\lambda_{1}$, the inequality holds and thus does for
    all eigenvalues. In other words,

    \[
      0 < \alpha < \frac{2}{\lambda_{i}}\quad i=\{ 1, 2, \ldots, n\}
    \]

    \item Express the condition on $\alpha$ in terms of the spectral condition
    number of $A$, $\kappa(A)$.

    {\bf Solution:}

    We can simply substitute $\kappa (A) = \frac{\lambda_{1}}{\lambda_{n}}$ into the above
    inequality, which gives

    \begin{align*}
      0 < \alpha < \frac{2}{\lambda_{n}\kappa (A)}
    \end{align*}

    \item Show that the best value for the step size in terms of maximizing the
    speed of convergence is

    \[
      \alpha = \frac{2}{\lambda_{1} + \lambda_{2}}
    \]

    Find the spectral radius of the iteration matrix in this case, and express
    it in terms of the condition number $A$.

    {\bf Solution:}

    We have that $T = \Id - \alpha A$, so we can compute $\kappa (T)$ as

    \begin{align*}
      \kappa (T) &= \frac{\abs{1 - \lambda_{1}\alpha}}{\abs{1 - \lambda_{n}\alpha}}
      \intertext{Where the best value for the step size would happen with $\kappa (T) = 1$, so}
      1 &= \frac{\abs{1 - \lambda_{1}\alpha}}{\abs{1 - \lambda_{n}\alpha}}\\
      \left( 1 - \lambda_{1}\alpha\right)^{2} &= \left( 1 - \lambda_{n}\alpha\right)^{2}\\
      \alpha^{2} \P{\lambda_{1}^{2} - \lambda_{n}^{2}} - 2\alpha\P{\lambda_{1} - \lambda_{n}} = 0\\
      \alpha^{2}\P{\lambda_{1} + \lambda_{n}} &= 2\alpha
    \end{align*}

    Since $\alpha > 0$, the above equation results in
    \[
    \alpha = \frac{2}{\lambda_{1} + \lambda_{n}}
    \]
    \item Determine whether the following statement is true or false. Justify
    your answer

    ``If $A$ is strictly diagonally dominant and $\alpha = 1$, then the iterative
    scheme converges to the solution for any initial guess $\mathbf{x}_{0}$.''

    {\bf Solution:}

    This can be done with an example

    \[
        A = \begin{pmatrix}3 & 0\\0 & 4 \end{pmatrix}
    \]

    Matrix $A$ is strictly diagonally dominant and also since it is diagonal its eigenvalues
    are $\{ 3, 4\}$, so it is positive semi-definite as well. With its largest eigenvalue being 4,
    for this to be convergent $\alpha$ needs to be restricted by $0 < \alpha < 1/4$. The question
    states that $\alpha = 1$, so therefore it is not convergent.
  \end{enumerate}
\end{enumerate}

%%%%%% Problem 5 %%%%%%
\item Consider the linear system $A\mathbf{x} = \mathbf{b}$, where $A$ is a symmetric
matrix. Suppose that $M - N$ is a splitting of $A$, where $M$ is symmetric positive
definite and $N$ is symmetric. Show that if $\lambda_{\minn}(M) > \rho(N)$, then
the iterative scheme $M\mathbf{x}_{k+1} = N\mathbf{x}_{k} + \mathbf{b}$ converges
to $\mathbf{x}$ for any initial guess $\mathbf{x}_{0}$.

{\bf Solution:}

The question states that $\lambda_{\minn} > \rho(N)$, so we can use the fact that

\begin{align}
  \lambda_{\minn}(M) &= \frac{1}{\lambda_{\maxx}\P{\inv{M}}}\nonumber\\
  \intertext{By substituting this equation into the previous, we get}
  \lambda_{\minn}(M) &> \rho(N)\nonumber\\
  &\Rightarrow 1 > \lambda_{\maxx}\P{\inv{M}}\rho(N)\nonumber\\
  &= \rho\P{\inv{M}}\rho(N)\nonumber\\
  &\Rightarrow 1 > \rho\P{\inv{M}}\rho(N)\label{eq:ineq_5}
\end{align}

Now, to show that it is convergent with any starting point

\begin{align*}
  Mx_{k+1} &= Nx_{k} + \vect{b}\\
  &\Rightarrow x_{k+1} = \inv{M}Nx_{k} + \inv{M}\vect{b}\\
  e_{k+1} &= \P{\inv{M}N}e_{k}\\
  &\Rightarrow \norm{e_{k+1}} = \norm{\inv{M}Ne_{k}}\\
  &\leq \norm{\inv{M}}\norm{N}\norm{e_{k}}\\
  \frac{\norm{e_{k+1}}}{\norm{e_{k}}} &\leq \norm{\inv{M}} \norm{N}
\end{align*}

Equation~\ref{eq:ineq_5} completes the proof, giving

\begin{align*}
  \norm{\inv{M}} \norm{N} &< 1\\
  &\Rightarrow \frac{\norm{e_{k+1}}}{\norm{e_{k}}} < 1
\end{align*}

%%%%%% Problem 6 %%%%%%
\item \
\begin{enumerate}
\item Write a program for solving the linear least squares problems that arise
throughout the iterations of the {\sc Gmres} method, using Givens rotations,
where the matrix is a non-square $(k+1) \times k$ upper Hessenberg matrix. Specifically,
solve

\[
\min_{\vect{z}} \norm{\rho e_{1} - H_{k+1,k}\mathbf{z}}
\]

Provide a detailed explanation of how this is done in your program, and state
what $Q$ and $R$ in the associated QR-Factorization are.

{\bf Solution:}

The upper Hessenberg matrix is formed via iterative QR Factorizations and is
of the form $H_{k} = Q_{k} R_{k}$, and can be formed from Givens Rotation Matrix.
Following which, we can get the solution to the minimization problem by solving
the system

\[
    R_{k} \vect{x}_{k} = Q_{k}^{T}\vect{b}
\]

which can be iterated over for to get the values. In our instance, $\rho = \vect{b} - A\vect{x}_{k}$
and $e_{1} = \left(1, 0, 0, \ldots, 0 \right)\in \mathbb{R}^{n}$. This gives the minimization problem
that we have to solve as being

\[
    R_{k}\vect{z} = Q_{k}^{T}\rho e_{1}
\]

Where the code for this can be found in {\tt prob6a.py}

\item Given $H_{k+1,k}$, suppose we perform a step of the Arnoldi process. As a
result, we now have a new upper Hessenberg matrix $H_{k+2,k+1}$. Describe the
relationship between the old and the new upper Hessenberg matrices and explain
how this relationship can be used to solve the new least squares problem in an
economical fashion.

{\bf Solution:}

The system produced in the minimization problem is

\[
  AQ_{n} = Q_{n+1} \widetilde{H}_{n}
\]

and the produced upper Hessenberg matrix is of dimensions $(m + 1) \times m$ while
the original matrix $AQ_{n}$ is $m \times n$. Therefore, it produces a smaller matrix
with the new iteration which can be used to save on computation time.

\item The least squares problems throughout the iterations can be solved using a
QR decomposition approach. Show that the upper triangular factor cannot be singular
unless $\mathbf{x}_{k} = \mathbf{x}$, the exact solution.

{\bf Solution: }

\begin{align*}
  A^{T}A\vect{x} &= A^{T}\vect{b}\\
  R^{T}Q^{T}QR\vect{x} &= R^{T}Q^{T}\vect{b}\\
  R^{T}R\vect{x} &= R^{T}Q^{T}\vect{b}\quad \P{Q^{T}Q = \Id}\\
  R\vect{x} &= Q^{T}\vect{b}\\
  \vect{x} &= \inv{R} Q^{T} \vect{b}
\end{align*}

where $R$ is non-singular, or else it wouldn't be invertable.


\end{enumerate}

%%%%%% Problem 7 %%%%%%
\item A column-stochastic matrix $P$ is a matrix whose entries are nonnegative
and whose column sums are all equal to $1$. In practice, such matrices are often
large and sparse.

Let $E$ be a matrix of the same size as $P$, say, $n\times n$, all of whose entries
are equal to $1/n$, and let $\alpha$ be a scalar, $0 < \alpha < 1$.

\begin{enumerate}
  \item Show that $A(\alpha) = \alpha P + (1-\alpha )E$ is also a column-stochastic matrix.

  {\bf Solution:}

  We can write any arbitrary element of each matrix in the above given equation, giving

  \begin{align*}
    A_{i,j} &= \alpha P_{i,j} + (1 - \alpha) E_{i,j}\\
    \intertext{Now in taking a summation of an arbitrary column from above}
    \sum_{i=1}^{n}A_{i,j} &= \alpha \sum_{i=1}^{n}P_{i,j} + (1-\alpha)\sum_{i=1}^{n}E_{i,j}\\
    &= \alpha + (1 - \alpha) = 1
  \end{align*}

  Which is due to the fact that $E$ and $P$ are column-stochastic matrices.

  \item What are the largest eigenvalue and corresponding eigenvector of $A(\alpha)$?

  {\bf Solution:}

  The text states that the largest eigenvalue of a {\em row-stochastic matrix} is 1. $A(\alpha)$
  is a {\em column-stochastic matrix}, so its transpose is a {\em row-stochastic matrix}
  with a largest eigenvalue of 1. Therefore, the largest eigenvalue of $A(\alpha)$ is
  also 1 since the eigenvalues of a matrix and its transpose are the same as it's based
  on the linear dependence of the rows/columns.

  \item Show that the second largeset eigenvalue of $A(\alpha)$ is bounded (in absolute value)
  by $\alpha$.

  {\bf Solution:}

    [Extracted from \url{http://nlp.stanford.edu/pubs/secondeigenvalue.pdf}]

    We know that the second eigenvector $\vect{x}_{2}$ of $A$ must be an eigenvector
    $\vect{y}_{i}$ of $P^{T}$, and the corresponding eigenvalue is $\gamma_{i} = \lambda_{2}/\alpha$.

    So, since $\lambda_{2} = \alpha \gamma_{i}$, and since we know $P$ is stochastic,
    we have $\abs{\gamma_{i}} \leq 1$, and therefore $\abs{\lambda_{2}} \leq \alpha$.


  \item Suppose the dominant eigenvector of $A(\alpha)$ is to be computed using
  the power method. This vector, if normalized so that its $\ell_{1}$-norm is
  equal to 1, is called the stationary distribution vector.

  \begin{enumerate}
    \item Show how matrix-vector products with $P(\alpha)$ can be performed in an
    efficient manner in terms of storage. (Assume $n$ is very large, and recall
    that $E$ is dense).

    {\bf Solution:}

    If $P(\alpha)$ has elemental values of $1/n$, a matrix vector multiplication can be computed
    efficiently in storage by simply storing the value of $\alpha /n$ in a single float, and
    the resulting scalar vector multiplicatio from $\alpha /n$ and a vector would produce the
    same results as if you did the enitre matrix vector multiplication since all elements
    in the matrix have the same value.

    If, they infact, do not have the same values, then you can store only the non-zero elements and their
    indices, and simply do a matrix vector multpilcation that way, where you would ignore all the zeros as
    they wouldn't produce any meaningful results. Something such as {\sc csr} matrix format.

    \item Show that if the power method is applied, then if the initial guess $\mathbf{v}_{0}$
    satisfies $\norm{\mathbf{v}_{0}}_{1} = 1$, then all subsequent iterates $\mathbf{v}_{k}$
    also have a unit $\ell_{1}$-norm, and hence there is no need to normalize throughout the iteration.

    {\bf Solution:}

      Since each row in $A$ is scaled to sum upto $n$, if each row in $\vect{v}_{0}$ is
      scaled to sum up to 1, so will the result of $A\cdot \vect{v}$. This is due
      to the fact that since each column will scale proportionally to those values in
      $\vect{v}$ such that they will also scale to 1. With this scaling, none of the
      norms will have to be taken since they'll be 1. In other words

      \begin{align*}
        \sum_{i=1}^{n}\P{A^{(k)}\vect{x}}_{i} = \sum_{i=1}^{n}\P{\vect{x}}_{i}
        \end{align*}
  \end{enumerate}
\end{enumerate}

%%%%%% Problem 8 %%%%%%
\item Consider the least squares problem

\[
\min_{x} \norm{\mathbf{b} - A\mathbf{x}}_{2}
\]

where we know that $A$ is ill-conditioned. Consider the {\em regularization} approach
that replaces the normal equations by the modified, better-conditioned system

\[
\left( A^{T}A + \gamma I\right)\mathbf{x}_{\gamma} = A^{T}\mathbf{b},
\]

where $\gamma > 0$ is a parameter.

\begin{enumerate}
  \item Show that $\kappa_{2}^{2}(A) \geq \kappa_{2}\left( A^{T}A + \gamma I \right)$

  {\bf Solution:}

  Since $A^{T}A$ is symmetrix positive-definite, its eigenvalues are such that
  $\lambda_{1} \geq \lambda_{2} \geq \cdots \geq \lambda_{n} \geq 0$. We have
  \begin{align*}
    A^{T}A\vect{x} &= \lambda\vect{x}\\
    &\Rightarrow A^{T}A\vect{x} + \gamma\vect{x} = \lambda\vect{x} + \gamma\vect{x}\\
    \vect{x} \P{A^{T}A + \gamma} &= \vect{x}\P{\lambda + \gamma}\\
    \kappa_{2}\P{A^{T}A + \gamma\Id} &= \frac{\lambda_{1} + \gamma}{\lambda_{n} + \gamma}\\
    \intertext{Since $\kappa^{2}(A) = \kappa\P{A^{T}A}$}
    \kappa^{2}(A) &= \frac{\lambda_{1}}{\lambda_{n}}\\
    \intertext{To prove $\kappa_{2}^{2} \geq \kappa_{2}\P{A^{T}A + \gamma\Id}$, we show that $\kappa_{2}^{2}(A) - \kappa_{2}\P{A^{T}A + \gamma\Id} \geq 0$}
    \kappa_{2}^{2}(A) - \kappa_{2}\P{A^{T}A + \gamma\Id} &= \frac{\lambda_{1}}{\lambda_{n}} - \frac{\lambda_{1} + \gamma}{\lambda_{n} + \gamma}\\
    &= \frac{\lambda_{1}\lambda_{n} + \lambda_{1}\gamma - \lambda_{1}\lambda_{n} - \lambda_{n}\gamma}{\P{\lambda_{n}}^{2} + \lambda_{n}\gamma}\\
    &= \frac{\P{\lambda_{1} - \lambda_{n}}\gamma}{\P{\lambda_{n}}^{2} + \lambda_{n}\gamma}
  \end{align*}

  which is greater than or equal to zero since $\gamma > 0$ and all eigenvalues are greater than or equal to zero.

  \item Reformulate the equations for $\mathbf{x}_{\gamma}$ as a linear least
  squares problem.

  {\bf Solution:}

  \begin{align*}
    \underbrace{\left( A^{T}A + \gamma I\right)}_{M}\mathbf{x}_{\gamma} &= A^{T}\mathbf{b}\\
    \underbrace{AM}_{W}\vect{x}_{\gamma} &= \underbrace{\P{AA^{T}}\vect{b}}_{\vect{d}}\\
    W\vect{x}_{\gamma} &= \vect{d}\\
    &\Rightarrow \min_{\vect{x}_{\gamma}}\norm{\vect{d} - W\vect{x}_{\gamma}}_{2}
  \end{align*}

  \item Show that $\norm{\mathbf{x}_{\gamma}}_{2} \leq \norm{\mathbf{x}}_{2}$.

  {\bf Solution:}

  From the two equations, we have
  \begin{align*}
    A^{T}A\vect{x}_{\gamma} + \gamma\Id\vect{x}_{\gamma} &= A^{T}A \vect{b}\\
    A^{T}A\vect{x} &= A^{T}\vect{b}\\
    \intertext{Where we can set these two equal to each other, with $A^{T}A = M$, giving}
    M\vect{x}_{\gamma} + \gamma\Id\vect{x}_{\gamma} &= M\vect{x}\\
    \vect{x}_{\gamma} + \inv{M}\gamma\Id\vect{x}_{\gamma} &= \vect{x}\\
    \vect{x} - \vect{x}_{\gamma} &= \inv{M}\gamma\Id\vect{x}_{\gamma}\\
    \vect{x} - \vect{x}_{\gamma} &\geq 0
  \end{align*}

 Where the last term comes about since $A$ is postive definite and the right hand side
 of the equation would be greater than zero. Since this is true, we can say
 $\norm{\mathbf{x}_{\gamma}}_{2} \leq \norm{\mathbf{x}}_{2}$.


  \item Find a bound for the relative error $\frac{\norm{\mathbf{x} - \mathbf{x}_{\gamma}}_{2}}{\norm{\mathbf{x}}_{2}}$
  in terms of either the largest or the smallest singular value of the matrix $A$.

  State a sufficient condition on the value of $\gamma$ that would guarentee that the relative
  error is bounded below a given value $\epsilon$.

  {\bf Solution:}

  \begin{align*}
    \vect{x}_{\alpha} &= \vect{x} - \inv{\P{A^{T}A}}\gamma\vect{x}_{\gamma}\\
    \vect{x} - \vect{x}_{\alpha} &= \inv{\P{A^{T}A}}\gamma\vect{x}_{\gamma}\\
    \norm{\vect{x} - \vect{x}_{\alpha}} &= \norm{\inv{\P{A^{T}A}}\gamma\vect{x}_{\gamma}}\\
              &\leq \gamma\norm{\inv{\P{A^{T}A}}}\norm{\vect{x}_{\gamma}}
    \intertext{But, since $\norm{\mathbf{x}_{\gamma}}_{2} \leq \norm{\mathbf{x}}_{2}$}
    \norm{\vect{x} - \vect{x}_{\alpha}} &\leq \gamma\norm{\inv{\P{A^{T}A}}}\norm{\vect{x}}\\
    \intertext{Finally}
    \frac{\norm{\vect{x} - \vect{x}_{\alpha}}}{\norm{\vect{x}}}  &\leq \gamma\norm{\inv{\P{A^{T}A}}}\\
    \intertext{We need the relative error to be less than $\epsilon$, so}
    \gamma\norm{\inv{\P{A^{T}A}}} &\leq \epsilon\\
      &\Rightarrow \gamma \leq \frac{\epsilon}{\norm{\inv{\P{A^{T}A}}}}
    \intertext{giving}
    \frac{\norm{\vect{x} - \vect{x}_{\alpha}}}{\norm{\vect{x}}}  &\leq \gamma\norm{\inv{\P{A^{T}A}}} \leq \epsilon\\
  \end{align*}

  \item Write a short program to solve the $5 \times 4$ problem in Example 8.8 regularized
  as above, using {\sc Matlab}'s backslash command. Try $\gamma = 10^{-j}$ for
  $j = \{ 0, 3, 6, 12\}$. For each $\gamma$, calculate the $\ell_{2}$-norms of the
  residual, $\norm{B\mathbf{x}_{\gamma} - \mathbf{b}}$, and the solution, $\norm{\mathbf{x}_{\gamma}}$.
  Compare to the results for $\gamma = 0$ and to those using SVD as reported in
  Example 8.8. What are your conclusions?

  {\bf Solution:}

  The matrix system that is being solved is $A\vect{x} = \vect{b}$ which is

  \[
        A = \begin{pmatrix}1 & 0 & 1 & 2\\
                           2 & 3 & 5 & 10\\
                           5 & 3 & -2& 6\\
                           3 & 5 & 4 & 12\\
                           -1 & 6 & 3 & 8\end{pmatrix}\quad \vect{x} = \begin{pmatrix}4\\-2\\5\\-2\\1\end{pmatrix}
  \]

  where we can solve for this system with regularization, which is stated in the proposed
  minimizatio problem above, and leads to

  \begin{align*}
    W^{T}W\vect{x}_{\gamma} &= W^{T}AA^{T}\vect{b}\\
    R^{T}QQ^{T}R\vect{x}_{\gamma} &= R^{T}Q^{T}AA^{T}\vect{b}\\
    \vect{x}_{\gamma} &= \inv{R}Q^{T}AA^{T}\vect{b}
  \end{align*}

  where the QR decomposition was taken against $W$. This solves the system based
  on the regularization technique discussed in this problem. In solving this system,
  we get the following results

  \begin{table}[H]
    \centering
    \begin{tabular}{c c c c}
      \hline\hline
      $j$ & $\norm{A\vect{x}_{\gamma} - \vect{b}}_{2}$ & $\norm{\vect{x}_{\gamma}}_{2}$ & $\norm{A^{T}A\vect{x}_{\gamma} - A^{T}\vect{b}}_{2}$\\
      \hline
      0 & 5.02675 & 1.82337 & 1.57531\\
      3 & 5.02505 & 3.31057 & 0.28440\\
      6 & 5.02519 & 0.95478 & 0.38355\\
      12 & 5.04030 & 1.87860 & 8.38387\\
      \hline
    \end{tabular}
  \end{table}

  which makes sense for the final column. As $j$ became larger, the $\lambda$ value became
  smaller and it approached closer to the {\em non-regularized} equation as it approached
  roundoff errors.

  For using {\sc Numpy}'s built in equation solver, {\tt numpy.linalg.solve}, we got\\
  $\norm{A\vect{x} - \vect{b}}_{2} = 5.02500$ and $\norm{\vect{x}}_{2} = 0.94733$

  \item For large ill-conditioned least squares problems, what is the potential advantage
  of the regularization with $\gamma$ presented here over minimum norm truncated SVD?

  {\bf Solution:}

  In calculating the SVD, the higher the condition number of the matrix, the better.
  With using a minimum norm truncated SVD on a matrix with a low condition number, the
  ratio between the max singular value and the minimum singular value is low. In doing a
  minimum norm, there is no stability on the number of eigenvalues that are being removed.
  For example, if you're looking for a norm truncation of 0.9, and all the values are below 1,
  you will have to remove a good majority of the eigenvalues to get the norm that low.

  With this method, it is independent on the removal of the eigenvalues and essentially
  ``feeds'' any error back into the system with $\gamma\Id$, and removes error. Also,
  there's no possibility of removing {\em all} of the eigenvalues in the system.

\end{enumerate}
\end{enumerate}

%\begin{proof}
%Blah, blah, blah.  Here is an example of the \texttt{align} environment:
%Note 1: The * tells LaTeX not to number the lines.  If you remove the *, be sure to remove it below, too.
%Note 2: Inside the align environment, you do not want to use $-signs.  The reason for this is that this is already a math environment. This is why we have to include \text{} around any text inside the align environment.
%\begin{align*}
%\sum_{i=1}^{k+1}i & = \left(\sum_{i=1}^{k}i\right) +(k+1)\\
%& = \frac{k(k+1)}{2}+k+1 & (\text{by inductive hypothesis})\\
%& = \frac{k(k+1)+2(k+1)}{2}\\
%& = \frac{(k+1)(k+2)}{2}\\
%& = \frac{(k+1)((k+1)+1)}{2}.
%\end{align*}
%\end{proof}

\end{document}
